---
title: "La révolution du machine learning"
output:
  html_document:
    toc: true
    toc_float: true
bibliography: /Users/thomas/Documents/Open/Rendu_individuel/biblio/Biblio1.bib
csl: "/Users/thomas/Documents/Open/Rendu_individuel/biblio/iso-690.csl"
---

# Machine Learning et réseaux de neurones

Dans ses débuts, l’intelligence artificielle était principalement constituée d’algorithmes basés sur des règles prédéfinies par des développeurs, qui permettaient d’automatiser un certain nombre de tâches précises. Dès les premières formes d’automatisation, on a pu assister à l’apparition d’inquiétudes sur une potentielle disparition des métiers, et sur une certaine une forme de déshumanisation, et de dépendance liée à une présence croissante des machines dans nos vies. Ces craintes, ne vont pas s’atténuer avec le temps, bien au contraire, surtout avec le développement plus récent de méthodes de machine learning sophistiquées. Turing avait imaginé le machine learning. En 1950, il dit que ce ne serais pas possible d’avoir une vraie IA si les humains devaient écrire eux même les algorithmes. Ce serait trop long si les humains écrivaient eux-mêmes. Le learning machine est donc le fait que l’IA se code seul.

## L'histoire de Garry Kasparov

Fin des années 90, IBM met au point une Intelligence Artificielle appelée Deep Blue, programmée pour jouer aux échecs. En 1996 a lieu un premier match contre le champion du monde de l'époque Garry Kasparov. Le champion gagne face à la machine 4-2. Mais en 1997 a lieu la revanche à New-York. Cette fois-ci, à la surprise générale, c'est le superordinateur IBM Deep Blue qui remporte la partie ! Il s'agit là de la première défaite de l'histoire d'un champion du monde face à une machine. Cet évènement a été symboliquement forte car elle était un signe que l'intelligence artificielle était en train de rattraper l'intelligence humaine, puisqu'elle devenait capable de battre le meilleur d'entre eux sur un jeu considéré comme très intellectuel à l'époque.[@franz_strasser_deep_2017]

![Kasparov contre Deep Blue](https://www.espace-turing.fr/local/cache-vignettes/L700xH394/http---com.ft.imagepublish.prod.s3.amazonaws.com-0f12cb56-3409-11e7-99bd-13beb0903fa3-9f29a.jpg?1688192020)




## IA vs. Machine Learning

Avant même de parler de machine learning, on parlait déjà d’intelligence artificielle. L’automatisation, les systèmes experts ou des algorithmes très basiques étaient identifiés comme étant de l’intelligence artificielle. Le Machine learning, lui, est une méthode particulière d’IA : il s’agit de l’apprentissage automatique. Toutes les IA ne s’appuient donc pas nécessairement sur du machine learning, mais le machine learning est un type d’IA particulier.

## Définition du machine learning

Le machine learning est le fait d’avoir une machine qui apprend. Définit dès la moitié du XX ème siècle Tom Mitchell (1997) dit qu’une machine va apprendre quand elle améliore ses performances en effectuant des tâches grâce à l’expérience. Il vaut mieux une IA basique qui s’améliore grâce à l’expérience qu’une machine sophistiquée.

## La notion d’apprentissage automatique

On peut distinguer deux approches de l’IA qui sont l’approche classique par programmation et l’approche par apprentissage automatique. L’approche classique par programmation aussi appelé rule-based AI est le principe qu’il y a une intervention humaine dans le processus. C’est la première approche de l’IA.Un programme est écrit en amont par un développeur humain. Le programme suit des règles pré-établi par celui-ci. Il prend des décisions ou éxecute des actions en fonction des entrées fourni. Les actions sont donc simple et limité. Cette approche est souvent utilisée dans des domaines où les problèmes sont bien définis et où les solutions peuvent être explicitement codées. Ses désavantages sont le manque de flexibilité, le besoin de mise à jour et la compléxité croissante au fur et à mesure que l’IA évolue. La seconde approche est l’apprentissage automatique aussi appelé data-driven AI. Cette approche consiste que l’IA apprend d’elle-même à partir de données. Les algorithmes d'apprentissage automatique sont alimentés avec des données d'entrée et des résultats souhaités (ou des étiquettes) pour apprendre des modèles ou des relations entre les entrées et les sorties. Le processus d'apprentissage implique généralement une phase d'entraînement où le modèle est exposé à un ensemble de données annotées, suivi d'une phase de test pour évaluer ses performances sur des données non vues auparavant. En ajustant les paramètres du modèle à partir des données d'entraînement, l'algorithme apprend à généraliser à de nouvelles situations. Cet IA présente de nombreux avantages tels que : la flexibilité, en detectant et s'adaptant à des modèles complexes dans les données, même lorsque les règles ne sont pas clairement définies par les humains. L’adaptabilité,les modèles peuvent être mis à jour et améliorés en continu à mesure que de nouvelles données deviennent disponibles, sans nécessiter de réécriture de règles manuelles. Et l’évolutivité : Les techniques d'apprentissage automatique peuvent être appliquées à de grands ensembles de données, ce qui permet de résoudre des problèmes à grande échelle. Cependant cet IA nécessite des données de qualités, de fournir des résultats durs à intepreter

## Les deux grandes phases du Machines Learning

La phase 1 consiste à l’apprentissage. L’objectif de cette première étape est de créer et calibrer un modèle d’IA en apprenant à partir d’un volume important de data. On parle aussi d’étape d’entrainement. L’apprentissage commence par le data.L’ensemble des data utilisées pour l’apprentissage. Ces données doivent être préparées en amont. Elles doivent être nettoyées, structurées et annotées. 80% du data est utilisé pour entrainer l’algorithme et 20% pour tester l’algorithme et faire des réajustement. Le data d’entrainement constitue le paramétrage du modèle. Un modèle de machine learning qui va s’ajuster pour permettre d’obtenir des résultats qui correspondent le mieux aux données utilisées pour l’entrainement. Puis on va comparer si pour des entrées données, les prédictions faites avec notre algorithme correspondent bien aux outputs réels observés pour ces entrées dans les 20% de notre base de données. En fonction des résultats, l’algorithme va s’ajuster. La phase 2 est l’inférence, on utilise le modèle obtenu dans la phase 1 pour faire des prédictions. De nouvelles données vont être utilisées pour faire des prédictions avec notre modèle. La prédiction obtenue en appliquant le modèle de machine learning qui a été entrainé. Pour s’assurer des bonnes performances il existe des indicateurs statistiques.

## Les deux types d’apprentissage

| Apprentissage supervisé      | Apprentissage non supervisé |
|-------------------------------|-----------------------------|
| On dit à l'algorithme ce qu'il doit trouver en labélisant les données. | L'algorithme ne sait pas ce qu'il doit trouver. |
| Fonctionnement: On lui donne des images de pommes, et on lui indique ensuite avec un label s'il s'agit bien d'une pomme ou non. A force de voir des images de pommes et de constater qu'elles sont associées au label "pomme", il va apprendre à reconnaitre des pommes. | De lui-même, il va trouver des groupes sans savoir à quoi chaque groupe correspond. Il n'y a aucune labélisation : l'algorithme va lui-même chercher à rassembler des élements qui se ressemblent. |
| Intérêts et objectifs: Faire des prédictions. Ce type d'apprentissage permet de faire des prédictions: Reconnaissance d'objets, Prédictions numériques, etc. | Faire de l'exploration. Ce type d'apprentissage permet de découvrir des catégories ou des patterns que l’on n’aurait pas envisagé. Il est intéressant pour explorer des données et découvrir des structures dans les données. |
| Algorithmes principaux: Classification, regression | Clustering,réduction dimensionnelle |

## Les réseaux de neurones

Les réseaux de neurones représentent une avancée majeure dans le domaine de l'intelligence artificielle. Contrairement aux approches traditionnelles qui reposent sur des règles prédéfinies, les réseaux de neurones adoptent une approche plus inspirée du fonctionnement du cerveau humain pour résoudre des problèmes complexes.

Pour comprendre le fonctionnement des réseaux de neurones, il est important de souligner qu'ils ne partent pas de zéro. Au contraire, ils nécessitent des structures mathématiques préalablement définies, avec des paramètres à estimer. Une des techniques utilisées pour ce faire est la programmation à l'aide de régressions linéaires, qui permettent d'ajuster les paramètres du réseau en fonction des données d'entrée et des résultats souhaités.

La véritable révolution des réseaux de neurones a commencé en 1957 avec le développement du perceptron, considéré comme le premier réseau de neurones artificiels. Le perceptron était capable d'apprendre à reconnaître des schémas simples en ajustant ses poids et ses seuils en réponse à des exemples d'entrée.

Un réseau de neurones est essentiellement une transmission de messages en binaire, où les entrées sont traitées par des neurones interconnectés qui transmettent des signaux à travers des poids synaptiques. Ces signaux sont ensuite combinés et transformés par des fonctions d'activation pour produire des sorties. Cette architecture en couches permet aux réseaux de neurones d'apprendre et de généraliser à partir de données d'entraînement, ce qui leur permet de résoudre une grande variété de tâches, de la reconnaissance d'images à la traduction automatique.
[@kelleher_john_d_deep_2019]


# Les trois ruptures de l’IA

Dans une récente note pour la Fondapol, Serge Soudoplatoff met en avant trois "grandes ruptures"quasi concomitantes qui ont permis à l'intelligence artificielle de franchir une grande étape ! 
1. Le Big Data : la disponibilité de très grandes bases de données correctement annotées permettant un apprentissage plus fin des algorithmes 
2. La capacité de calcul augmentée : l'arrivée sur le marché de processeurs graphiques à bas coûts capables d'effectuer d'énormes quantités de calculs. 
3. Le développement de nouveaux algorithmes : l'introduction d'une catégorie d'algorithmes bien plus sophistiqués tels que les réseaux de neurones convolutifs notamment.
[@serge_soudoplatoff_intelligence_2018]

# Du Machine Learning au Deep Learning

## Les réseaux de neurones profonds

Dans un réseau de neurones nous avons des entrées et des sorties. Quand il y a beaucoup d’entrée cela devient compliqué. Alors dans un premier temps nous listons les caractéristiques (humain) ensuite nous appliquons un algorithme d’abstraction de l’image qui va identifier ou non les caractéristiques. Par la suite nous pouvons faire du machine learning. Cela réduit les entrées. Mais la limite est les caractéristiques elles-mêmes. Le deep learning va révolutionner la reconnaissance d’image. Les neurones vont comprendre tout seul ce qu’est un chat en s’entrainant sur des images. Yann Lecun a amélioré en proposant des algorithmes et des processeurs plus puissants. Les plus connus sont les réseaux convolutionnels. Le deep learning fonctionne comme un réseau de neurones normal mais avec plus de couche. L’IA défini elle-même les caractéristiques de reconnaissance puis s’entraine.

## Le deep learning révolutionne tous les secteurs

De façon très concrète, le développement du Deep Learning a grandement impacté les capacités des Intelligences Artificielles, qui ont progressivement impacté un très grand nombre de secteurs !

Le Deep Learning est utilisé dans la médecine prédictive pour analyser de grandes quantités de données médicales, telles que les dossiers de santé électroniques, les résultats de tests d'imagerie médicale (comme les scanners et les IRM), les données génomiques, etc. L’IA sert donc d’assistant et d’outils d’aide à la décision pour les médecins. Ces données sont utilisées pour prédire les risques de maladies, diagnostiquer des conditions médicales, recommander des traitements personnalisés, et même prédire l'efficacité des médicaments pour un patient spécifique. Par exemple, des modèles de Deep Learning sont utilisés pour prédire le risque de développer des maladies chroniques telles que le diabète ou les maladies cardiovasculaires à partir des données des patients.
[@jean_charlet_intelligence_2018]

La voiture autonome n’est plus une utopie grâce à l’IA. En effet, le Deep Learning est au cœur du développement des véhicules autonomes. Les algorithmes de Deep Learning sont utilisés pour analyser en temps réel les données provenant des capteurs des véhicules, tels que les caméras, les capteurs lidar et radar, pour détecter les obstacles, les piétons, les panneaux de signalisation, etc. Des entreprises telles que Waymo, Tesla, et Uber utilisent activement le Deep Learning dans leurs technologies de conduite autonome.
[@timothee_sainte_fare_garnot_quand_2020]

Dans le domaine du sport, le Deep Learning est utilisé pour analyser les performances des athlètes. Par exemple, des systèmes basés sur le Deep Learning sont utilisés pour suivre et analyser les mouvements des athlètes à partir de vidéos ou de capteurs portables, afin de fournir des informations sur leur technique, leur forme physique, et même pour prédire leurs performances lors de compétitions. Cette analyse peut aider les entraîneurs et les athlètes à identifier les domaines à améliorer et à optimiser leurs stratégies d'entraînement. Il permet même de faire des prédictions des compétitions à venir et donc aux équipes à mieux se préparer selon les adversaires.
[@noauthor_big_2019]

Dans ces domaines, le Deep Learning est utilisé pour diverses applications. En justice, il est utilisé pour l'analyse de grandes quantités de données judiciaires pour identifier des tendances, prédire les résultats des affaires judiciaires, ou même pour aider à la prise de décision dans les tribunaux. Dans l'art, le Deep Learning est utilisé pour générer des œuvres d'art, créer des effets visuels et spéciaux dans les films, ou même pour analyser et classer des œuvres d'art. La génération d’images est notamment popularisée à travers l’interface de openAI: dall-e.
En musique, il est utilisé pour la génération de musique, l'analyse des préférences des auditeurs, et même pour la création d'outils de composition musicale assistée par ordinateur.

![Voiture autonome](https://dataanalyticspost.com/wp-content/uploads/2018/05/Voiture_autonome-1.jpg)


# L’enjeu du manque d’interprétabilité

## Le deep learning : de véritables boites noires

La boite noire est le fait que l’on trouve que c’est difficile de trouver le cheminement de l’algorithme car il est trop complexe. Les résultats obtenus sont obtenus trop difficilement. L’IA doit être capable d’expliquer ses résultats. Ce qui fait de ce problème l’un des majeurs concernant le deep learning. Par exemple si une voiture autonome percute un piéton alors qu’elle devait freiner, nous devons corriger cette erreur cependant nous n’arrivons pas à retracer le processus de prise de décision du véhicule. Ce problème pose une question sur la confiance que nous devons accorder aux intelligences artificielles. Sans recul sur ce que fait l’IA nous pouvons seulement suivre ce qu’elle fait sans prendre en compte qu’elle a pu subir des biais indésirables. Dans le domaine de la médecine le médecin de tenir compte de tous les details fournis par l’IA pour prendre une décision. C’est pour cela que le problème de la boite noire a été résolu.[@lou_blouin_ais_2023]

## Les boites blanches et le reverse engineering

Option 1 : Boîte blanche, intégration de l'interprétabilité dans l'algorithme

Dans cette approche, l'interprétabilité est directement intégrée dans la conception des algorithmes d'intelligence artificielle. Plutôt que de créer des modèles opaques et difficiles à comprendre, les développeurs travaillent à concevoir des modèles dont les décisions peuvent être facilement expliquées. Cela peut être réalisé en utilisant des techniques telles que des arbres de décision simples, des modèles linéaires ou des ensembles de règles logiques. L'avantage de cette approche est qu'elle rend les modèles plus faciles à comprendre dès le départ, ce qui peut favoriser la confiance des utilisateurs et faciliter la détection et la correction des erreurs ou des biais.

Option 2 : Algorithmes d'explicabilité (reverse engineering)

Dans cette approche, on utilise des techniques d'intelligence artificielle pour comprendre et expliquer les décisions prises par d'autres modèles d'intelligence artificielle. Cela peut impliquer l'utilisation de techniques d'interprétabilité, telles que la génération de règles ou d'explications à partir des prédictions d'un modèle, ou l'utilisation de méthodes d'analyse des gradients pour comprendre quels aspects des données ont le plus influencé les décisions du modèle. L'avantage de cette approche est qu'elle peut être appliquée à des modèles déjà existants, même s'ils ont été conçus sans prendre en compte l'interprétabilité. Cependant, cela peut nécessiter des ressources computationnelles importantes et peut ne pas toujours produire des explications faciles à comprendre pour les utilisateurs finaux.

# Conclusion
L’apparition du deep learning a révolutionné les IA en les rendant encore plus utiles et capable de tout prédire. Elle révolutionne tous les secteurs et constitue des avancés majeures dans ceux-ci. Elles tendent vers un changement radical du quotidien de chaque être humain. Cependant des questionnements apparaissent notamment sur l’éthique et la gestion de données.

# Bibliographie